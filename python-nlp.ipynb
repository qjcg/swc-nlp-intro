{
 "metadata": {
  "name": "",
  "signature": "sha256:9e3410283fba5fa9c2d4ee665f964863ce762e49644dc90eeac4c671413c3bb2"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Natural Language Processing with Python\n",
      "\n",
      "## Objectives\n",
      "\n",
      "- explain what NLTK is\n",
      "- split text into words and sentences (tokenization)\n",
      "- tag parts of speech"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# What is NLTK?\n",
      "\n",
      "- [Natural Language Toolkit (NLTK)](http://www.nltk.org)\n",
      "    - installed as part of Anaconda\n",
      "    - Book: [\"Natural Language Processing with Python\"](http://www.nltk.org/book/) available for free online"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# First Steps"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "import nltk"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# See what is provided with NLTK\n",
      "nltk.download()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let's install a few things by running the cell below.\n",
      "\n",
      "- [Brown corpus](http://en.wikipedia.org/wiki/Brown_Corpus) (\"brown\")\n",
      "    - Brown University Standard Corpus of Present-Day (1961) American English\n",
      "    - 500 samples of English-language text, totaling roughly one million words\n",
      "- A sentence tokenizer (\"punkt\")\n",
      "- A parts of speech tagger (\"maxent_treebank_pos_tagger\")"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "nltk.download(['brown', 'maxent_treebank_pos_tagger', 'punkt'])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Tokenization\n",
      "Let's create a list of a few english pangrams (if interested, more [here](http://clagnut.com/blog/2380#Longer_pangrams_in_English_.28in_order_of_fewest_letters_used.29))."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "pangrams = [\n",
      "    \"Woven silk pyjamas exchanged for blue quartz. (And more!)\",\n",
      "    \"A wizard's job is to vex chumps quickly in fog. (Other stuff...)\",\n",
      "    \"As quirky joke, chefs won't pay devil magic zebra tax.\",\n",
      "    \"My faxed joke won a pager in the cable TV quiz show.\"\n",
      "]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Tokenize words"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for p in pangrams:\n",
      "    print(nltk.tokenize.word_tokenize(p))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Tokenize sentences"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for p in pangrams:\n",
      "    print(nltk.tokenize.sent_tokenize(p))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# POS tagging"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "tokens = nltk.word_tokenize(pangrams[0])\n",
      "nltk.pos_tag(tokens)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Getting text from the web"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from bs4 import BeautifulSoup\n",
      "import requests\n",
      "\n",
      "txt = requests.get('https://news.google.com').text\n",
      "soup = BeautifulSoup(txt)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Print all titles\n",
      "#soup.select('span.titletext')\n",
      "\n",
      "title_elements = soup.select('span.titletext')\n",
      "for t in title_elements:\n",
      "    print(t.text)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}