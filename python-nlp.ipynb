{
 "metadata": {
  "name": "",
  "signature": "sha256:728d5c755e70bd9609b5d3448ea61a273d897d824af2745a7efc581dee8366f2"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Natural Language Processing with Python\n",
      "\n",
      "## Objectives\n",
      "\n",
      "- explain what NLTK is\n",
      "- split text into words and sentences (tokenization)\n",
      "- tag parts of speech"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# What is NLTK?\n",
      "\n",
      "- [Natural Language Toolkit (NLTK)](http://www.nltk.org)\n",
      "    - installed as part of Anaconda\n",
      "    - Book: [\"Natural Language Processing with Python\"](http://www.nltk.org/book/) available for free online"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# First Steps"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "import nltk\n",
      "\n",
      "# See what is provided with NLTK\n",
      "nltk.download()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let's install a few things by running the cell below.\n",
      "\n",
      "- [Brown corpus](http://en.wikipedia.org/wiki/Brown_Corpus) (\"brown\")\n",
      "    - Brown University Standard Corpus of Present-Day American English\n",
      "    - 500 samples of English-language text, totaling roughly one million words\n",
      "- A sentence tokenizer (\"punkt\")\n",
      "- A parts of speech tagger (\"maxent_treebank_pos_tagger\")"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "nltk.download(['brown', 'maxent_treebank_pos_tagger', 'punkt', 'universal_tagset'])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Tokenization\n",
      "Let's create a list of a few english pangrams (if interested, more [here](http://clagnut.com/blog/2380#Longer_pangrams_in_English_.28in_order_of_fewest_letters_used.29))."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "pangrams = [\n",
      "    \"Woven silk pyjamas exchanged for blue quartz.\",\n",
      "    \"A wizard's job is to vex chumps quickly in fog.\",\n",
      "    \"As quirky joke, chefs won't pay devil magic zebra tax.\",\n",
      "    \"My faxed joke won a pager in the cable TV quiz show.\"\n",
      "]\n",
      "\n",
      "print('TOKENIZED WORDS')\n",
      "for p in pangrams:\n",
      "    print(nltk.tokenize.word_tokenize(p))\n",
      "    \n",
      "for p in pangrams:\n",
      "    print(nltk.tokenize.sent_tokenize(p))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# POS tagging"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "tokens = nltk.word_tokenize(pangrams[0])\n",
      "nltk.pos_tag(tokens)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}